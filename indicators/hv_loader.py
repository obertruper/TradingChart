#!/usr/bin/env python3
"""
Historical Volatility (HV) Loader with Single-Pass Batch Processing
====================================================================
–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ Historical Volatility —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ (7, 14, 30, 60, 90)
- –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (HV Ratio, HV Percentile)
- –ë–∞—Ç—á–µ–≤–æ–π –∑–∞–ø–∏—Å–∏ –¥–∞–Ω–Ω—ã—Ö (—Ä–∞—Å—á–µ—Ç –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ, –∑–∞–ø–∏—Å—å –±–∞—Ç—á–∞–º–∏)
- –õ—é–±—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (1m, 15m, 1h –∏ —Ç.–¥.)
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π

Historical Volatility = StdDev(log returns) √ó ‚àö(periods_per_year) √ó 100%
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datetime import datetime, timedelta, timezone
import logging
from typing import Dict, List, Optional, Tuple
import pandas as pd
import numpy as np
import yaml
from tqdm import tqdm
import argparse
import warnings
import time

warnings.filterwarnings('ignore')

from indicators.database import DatabaseConnection


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –≥–æ–¥—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
ANNUALIZATION_FACTORS = {
    '1m': np.sqrt(525600),    # 60 * 24 * 365 = 525600 –º–∏–Ω—É—Ç –≤ –≥–æ–¥—É, ‚àö ‚âà 725
    '15m': np.sqrt(35040),    # 4 * 24 * 365 = 35040 15-–º–∏–Ω—É—Ç–æ–∫ –≤ –≥–æ–¥—É, ‚àö ‚âà 187
    '1h': np.sqrt(8760),      # 24 * 365 = 8760 —á–∞—Å–æ–≤ –≤ –≥–æ–¥—É, ‚àö ‚âà 93.6
    '4h': np.sqrt(2190),      # 6 * 365 = 2190 4-—á–∞—Å–æ–≤–æ–∫ –≤ –≥–æ–¥—É, ‚àö ‚âà 46.8
    '1d': np.sqrt(365),       # 365 –¥–Ω–µ–π –≤ –≥–æ–¥—É, ‚àö ‚âà 19.1
}

# –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è percentile —Ä–∞—Å—á—ë—Ç–∞ (–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞)
# 7 –¥–Ω–µ–π –∏ 90 –¥–Ω–µ–π
PERCENTILE_PERIODS = {
    '1m': {'7d': 10080, '90d': 129600},    # 7*24*60, 90*24*60
    '15m': {'7d': 672, '90d': 8640},       # 7*24*4, 90*24*4
    '1h': {'7d': 168, '90d': 2160},        # 7*24, 90*24
    '4h': {'7d': 42, '90d': 540},          # 7*6, 90*6
    '1d': {'7d': 7, '90d': 90},            # 7, 90
}


def setup_logging():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å"""
    log_dir = os.path.join(os.path.dirname(__file__), 'logs')
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f'hv_{timestamp}.log')

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    file_handler = logging.FileHandler(log_file)
    file_handler.setFormatter(formatter)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    logger.info(f"üìù HV Loader: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")
    return logger


logger = setup_logging()


class HVLoader:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ Historical Volatility –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
    """

    # –ö–æ–ª–æ–Ω–∫–∏ HV –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤ –ë–î
    HV_COLUMNS = [
        'hv_7', 'hv_14', 'hv_30', 'hv_60', 'hv_90',
        'hv_ratio_7_30', 'hv_percentile_7d', 'hv_percentile_90d'
    ]

    def __init__(self, symbol: str = 'BTCUSDT'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
        """
        self.db = DatabaseConnection()
        self.symbol = symbol
        self.config = self.load_config()
        self.symbol_progress = ""
        self.timeframe_minutes = self._parse_timeframes()
        self.force_reload = False

    def load_config(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Ñ–∞–π–ª–∞"""
        config_path = os.path.join(os.path.dirname(__file__), 'indicators_config.yaml')
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config

    def _parse_timeframes(self) -> dict:
        """–ü–∞—Ä—Å–∏—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        timeframe_map = {}
        timeframes = self.config.get('timeframes', ['1m'])

        for tf in timeframes:
            import re
            match = re.match(r'^(\d+)([mhdw])$', tf.lower())
            if match:
                number = int(match.group(1))
                unit = match.group(2)
                if unit == 'm':
                    timeframe_map[tf] = number
                elif unit == 'h':
                    timeframe_map[tf] = number * 60
                elif unit == 'd':
                    timeframe_map[tf] = number * 1440
                elif unit == 'w':
                    timeframe_map[tf] = number * 10080

        return timeframe_map

    def create_hv_columns(self, timeframe: str) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è HV –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
        table_name = f'indicators_bybit_futures_{timeframe}'

        with self.db.get_connection() as conn:
            cur = conn.cursor()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            cur.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_name = %s
                )
            """, (table_name,))

            if not cur.fetchone()[0]:
                logger.error(f"‚ùå –¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
                return False

            # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            cur.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = %s
            """, (table_name,))

            existing_columns = {row[0] for row in cur.fetchall()}

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            columns_added = []
            for col_name in self.HV_COLUMNS:
                if col_name not in existing_columns:
                    cur.execute(f"""
                        ALTER TABLE {table_name}
                        ADD COLUMN IF NOT EXISTS {col_name} DECIMAL(10,4)
                    """)
                    columns_added.append(col_name)

            if columns_added:
                conn.commit()
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ HV: {columns_added} –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}")
            else:
                logger.info(f"‚ÑπÔ∏è –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏ HV —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ {table_name}")

            return True

    def clear_hv_columns(self, timeframe: str) -> bool:
        """–û–±–Ω—É–ª—è–µ—Ç –≤—Å–µ HV —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –∏ —Å–∏–º–≤–æ–ª–∞"""
        table_name = f'indicators_bybit_futures_{timeframe}'

        with self.db.get_connection() as conn:
            cur = conn.cursor()

            try:
                set_clauses = [f'{col} = NULL' for col in self.HV_COLUMNS]
                set_clause = ', '.join(set_clauses)

                query = f"""
                    UPDATE {table_name}
                    SET {set_clause}
                    WHERE symbol = %s
                """

                cur.execute(query, (self.symbol,))
                rows_affected = cur.rowcount

                conn.commit()
                logger.info(f"üóëÔ∏è  –û–±–Ω—É–ª–µ–Ω–æ {rows_affected:,} –∑–∞–ø–∏—Å–µ–π –¥–ª—è HV —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ {table_name}")

                return True

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ HV —Å—Ç–æ–ª–±—Ü–æ–≤: {e}")
                conn.rollback()
                return False

    def get_null_timestamps(self, timeframe: str) -> set:
        """
        –ü–æ–ª—É—á–∞–µ—Ç timestamps –≥–¥–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ HV –∫–æ–ª–æ–Ω–∫–∞ IS NULL.
        """
        table_name = f'indicators_bybit_futures_{timeframe}'

        null_conditions = ' OR '.join([f'{col} IS NULL' for col in self.HV_COLUMNS])

        query = f"""
            SELECT timestamp
            FROM {table_name}
            WHERE symbol = %s
            AND ({null_conditions})
        """

        with self.db.get_connection() as conn:
            cur = conn.cursor()
            cur.execute(query, (self.symbol,))
            return {row[0] for row in cur.fetchall()}

    def calculate_hv(self, closes: np.ndarray, period: int) -> np.ndarray:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Historical Volatility –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞.

        HV = StdDev(log returns, period) √ó annualization_factor √ó 100

        Args:
            closes: –ú–∞—Å—Å–∏–≤ —Ü–µ–Ω –∑–∞–∫—Ä—ã—Ç–∏—è
            period: –ü–µ—Ä–∏–æ–¥ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞

        Returns:
            –ú–∞—Å—Å–∏–≤ HV –∑–Ω–∞—á–µ–Ω–∏–π (–≤ %)
        """
        if len(closes) < period + 1:
            return np.full(len(closes), np.nan)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float64
        closes = np.asarray(closes, dtype=np.float64)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º log returns
        log_returns = np.log(closes[1:] / closes[:-1])

        # Prepend NaN to align with closes
        log_returns = np.concatenate([[np.nan], log_returns])

        # Rolling standard deviation
        hv_values = np.full(len(closes), np.nan)

        for i in range(period, len(closes)):
            window = log_returns[i - period + 1:i + 1]
            if not np.any(np.isnan(window)):
                hv_values[i] = np.std(window, ddof=1)  # Sample std dev

        return hv_values

    def calculate_hv_ratio(self, hv_short: np.ndarray, hv_long: np.ndarray) -> np.ndarray:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç HV Ratio = HV_short / HV_long

        Args:
            hv_short: –ú–∞—Å—Å–∏–≤ HV –¥–ª—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
            hv_long: –ú–∞—Å—Å–∏–≤ HV –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞

        Returns:
            –ú–∞—Å—Å–∏–≤ HV Ratio
        """
        with np.errstate(divide='ignore', invalid='ignore'):
            ratio = hv_short / hv_long
            ratio = np.where(np.isinf(ratio), np.nan, ratio)
        return ratio

    def calculate_hv_percentile(self, hv_values: np.ndarray, lookback: int) -> np.ndarray:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç HV Percentile - —Ä–∞–Ω–≥ —Ç–µ–∫—É—â–µ–≥–æ HV –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—Ä–∏–∏.

        Percentile = (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π < —Ç–µ–∫—É—â–µ–≥–æ) / lookback √ó 100

        Args:
            hv_values: –ú–∞—Å—Å–∏–≤ HV –∑–Ω–∞—á–µ–Ω–∏–π
            lookback: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

        Returns:
            –ú–∞—Å—Å–∏–≤ HV Percentile (0-100)
        """
        percentile = np.full(len(hv_values), np.nan)

        for i in range(lookback, len(hv_values)):
            if np.isnan(hv_values[i]):
                continue

            # –ü–æ–ª—É—á–∞–µ–º –æ–∫–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            window = hv_values[i - lookback:i]

            # –£–¥–∞–ª—è–µ–º NaN
            valid_window = window[~np.isnan(window)]

            if len(valid_window) > 0:
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ–Ω—å—à–µ —Ç–µ–∫—É—â–µ–≥–æ
                count_less = np.sum(valid_window < hv_values[i])
                percentile[i] = (count_less / len(valid_window)) * 100

        return percentile

    def load_all_data(
        self,
        timeframe: str,
        max_lookback: int,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –¥–∞–Ω–Ω—ã–µ —Å–≤–µ—á–µ–π –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ HV.

        Args:
            timeframe: '1m', '15m', –∏–ª–∏ '1h'
            max_lookback: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π lookback –¥–ª—è percentile
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞

        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: timestamp, close
        """
        minutes = self.timeframe_minutes[timeframe]

        # Lookback –¥–ª—è percentile 90 –¥–Ω–µ–π + –±—É—Ñ–µ—Ä –¥–ª—è HV –ø–µ—Ä–∏–æ–¥–æ–≤
        lookback_minutes = max_lookback * minutes + 100 * minutes

        if start_date:
            adjusted_start = start_date - timedelta(minutes=lookback_minutes)
        else:
            adjusted_start = None

        if end_date is None:
            end_date = datetime.now(timezone.utc)

        with self.db.get_connection() as conn:
            cur = conn.cursor()

            if timeframe == '1m':
                query = """
                    SELECT timestamp, close
                    FROM candles_bybit_futures_1m
                    WHERE symbol = %s
                """
                params = [self.symbol]

                if adjusted_start:
                    query += " AND timestamp >= %s"
                    params.append(adjusted_start)
                if end_date:
                    query += " AND timestamp <= %s"
                    params.append(end_date)

                query += " ORDER BY timestamp"
                cur.execute(query, params)

            else:
                # –î–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
                if adjusted_start:
                    query_adjusted_start = adjusted_start - timedelta(minutes=minutes)
                else:
                    query_adjusted_start = None

                if minutes == 60:  # 1h
                    query = """
                        SELECT
                            date_trunc('hour', timestamp) as period_start,
                            (array_agg(close ORDER BY timestamp DESC))[1] as close_price
                        FROM candles_bybit_futures_1m
                        WHERE symbol = %s
                    """
                    params = [self.symbol]

                    if query_adjusted_start:
                        query += " AND timestamp >= %s"
                        params.append(query_adjusted_start)
                    if end_date:
                        query += " AND timestamp <= %s"
                        params.append(end_date)

                    query += """
                        GROUP BY date_trunc('hour', timestamp)
                        ORDER BY period_start
                    """

                else:  # 15m
                    query = f"""
                        SELECT
                            date_trunc('hour', timestamp) +
                            INTERVAL '{minutes} minutes' * (EXTRACT(MINUTE FROM timestamp)::INTEGER / {minutes}) as period_start,
                            (array_agg(close ORDER BY timestamp DESC))[1] as close_price
                        FROM candles_bybit_futures_1m
                        WHERE symbol = %s
                    """
                    params = [self.symbol]

                    if query_adjusted_start:
                        query += " AND timestamp >= %s"
                        params.append(query_adjusted_start)
                    if end_date:
                        query += " AND timestamp <= %s"
                        params.append(end_date)

                    query += f"""
                        GROUP BY date_trunc('hour', timestamp),
                                 EXTRACT(MINUTE FROM timestamp)::INTEGER / {minutes}
                        ORDER BY period_start
                    """

                cur.execute(query, params)

            rows = cur.fetchall()

            if not rows:
                return pd.DataFrame()

            df = pd.DataFrame(rows, columns=['timestamp', 'close'])

            logger.info(f"   üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å–≤–µ—á–µ–π –¥–ª—è {timeframe} (–≤–∫–ª—é—á–∞—è lookback)")
            if start_date:
                lookback_count = len(df[df['timestamp'] < start_date])
                data_count = len(df[df['timestamp'] >= start_date])
                logger.info(f"      ‚Ä¢ Lookback: {lookback_count:,} —Å–≤–µ—á–µ–π")
                logger.info(f"      ‚Ä¢ –î–∞–Ω–Ω—ã–µ: {data_count:,} —Å–≤–µ—á–µ–π")

            return df

    def process_timeframe(self, timeframe: str, batch_days: int = 1,
                         start_date: Optional[datetime] = None):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç HV –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞.

        Args:
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º ('1m', '15m', '1h')
            batch_days: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö (–¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î)
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–∏–æ–¥—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        hv_config = self.config.get('indicators', {}).get('hv', {})
        periods = hv_config.get('periods', [7, 14, 30, 60, 90])
        batch_days = hv_config.get('batch_days', batch_days)

        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ HV –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {timeframe}")
        logger.info(f"üìà –ü–µ—Ä–∏–æ–¥—ã HV: {periods}")
        logger.info(f"üéØ –°–∏–º–≤–æ–ª: {self.symbol}")
        logger.info(f"üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∑–∞–ø–∏—Å–∏: {batch_days} –¥–Ω–µ–π")

        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not self.create_hv_columns(timeframe):
            return

        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ force-reload
        if self.force_reload:
            logger.info(f"\nüîÑ –í–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º force-reload - –æ–±–Ω—É–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö HV –¥–∞–Ω–Ω—ã—Ö")
            if not self.clear_hv_columns(timeframe):
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω—É–ª–∏—Ç—å HV —Å—Ç–æ–ª–±—Ü—ã –¥–ª—è {timeframe}")
                return

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
        end_date = datetime.now(timezone.utc)

        if start_date is None:
            with self.db.get_connection() as conn:
                cur = conn.cursor()
                cur.execute("""
                    SELECT MIN(timestamp)
                    FROM candles_bybit_futures_1m
                    WHERE symbol = %s
                """, (self.symbol,))
                result = cur.fetchone()
                if result and result[0]:
                    start_date = result[0]
                else:
                    logger.error(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {self.symbol}")
                    return

        logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {start_date.date()} ‚Üí {end_date.date()}")

        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è percentile
        percentile_periods = PERCENTILE_PERIODS.get(timeframe, {'7d': 168, '90d': 2160})
        max_lookback = percentile_periods['90d']

        # –®–ê–ì 1: –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ
        logger.info(f"\nüîÑ –®–ê–ì 1/3: –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
        df = self.load_all_data(timeframe, max_lookback, start_date, end_date)

        if df.empty:
            logger.error(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return

        # –®–ê–ì 2: –†–∞—Å—á—ë—Ç HV –¥–ª—è –≤—Å–µ—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        logger.info(f"\nüîÑ –®–ê–ì 2/3: –†–∞—Å—á—ë—Ç HV –¥–ª—è –≤—Å–µ—Ö –ø–µ—Ä–∏–æ–¥–æ–≤...")

        closes = df['close'].values

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∞–Ω–Ω—É–∞–ª–∏–∑–∞—Ü–∏–∏
        annualization = ANNUALIZATION_FACTORS.get(timeframe, np.sqrt(8760))

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º HV –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        hv_results = {}
        for period in tqdm(periods, desc=f"{self.symbol} {timeframe} - –†–∞—Å—á—ë—Ç HV"):
            hv_raw = self.calculate_hv(closes, period)
            # –ê–Ω–Ω—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            hv_results[period] = hv_raw * annualization * 100

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        logger.info(f"   üìà –†–∞—Å—á—ë—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")

        # HV Ratio (7/30)
        hv_ratio_7_30 = self.calculate_hv_ratio(hv_results[7], hv_results[30])

        # HV Percentile (7 –¥–Ω–µ–π –∏ 90 –¥–Ω–µ–π)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º HV_30 –∫–∞–∫ –±–∞–∑–æ–≤—ã–π –¥–ª—è percentile
        hv_percentile_7d = self.calculate_hv_percentile(
            hv_results[30], percentile_periods['7d']
        )
        hv_percentile_90d = self.calculate_hv_percentile(
            hv_results[30], percentile_periods['90d']
        )

        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞–Ω–Ω—ã—Ö
        df_write = df[df['timestamp'] >= start_date].copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ HV –∫–æ–ª–æ–Ω–∫–∏
        for period in periods:
            full_hv = hv_results[period]
            actual_hv = full_hv[len(full_hv) - len(df_write):]
            df_write[f'hv_{period}'] = actual_hv

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        df_write['hv_ratio_7_30'] = hv_ratio_7_30[len(hv_ratio_7_30) - len(df_write):]
        df_write['hv_percentile_7d'] = hv_percentile_7d[len(hv_percentile_7d) - len(df_write):]
        df_write['hv_percentile_90d'] = hv_percentile_90d[len(hv_percentile_90d) - len(df_write):]

        logger.info(f"   ‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ HV –¥–ª—è {len(df_write):,} —Å–≤–µ—á–µ–π")

        # –®–ê–ì 3: –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î
        logger.info(f"\nüîÑ –®–ê–ì 3/3: –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ë–î...")

        total_records = len(df_write)
        if self.force_reload:
            logger.info(f"   üîÑ –†–µ–∂–∏–º force-reload: –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Å–µ {total_records:,} –∑–∞–ø–∏—Å–µ–π")
            records_to_write = df_write
        else:
            null_timestamps = self.get_null_timestamps(timeframe)

            if not null_timestamps:
                logger.info(f"   ‚úÖ –í—Å–µ HV –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã –¥–ª—è {timeframe} - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å")
                logger.info(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ HV –¥–ª—è {timeframe} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                return

            records_to_write = df_write[df_write['timestamp'].isin(null_timestamps)]
            skipped_records = total_records - len(records_to_write)

            logger.info(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
            logger.info(f"      ‚Ä¢ –í—Å–µ–≥–æ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ: {total_records:,} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"      ‚Ä¢ –ù—É–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å: {len(records_to_write):,} –∑–∞–ø–∏—Å–µ–π (HV IS NULL)")
            logger.info(f"      ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_records:,} –∑–∞–ø–∏—Å–µ–π (—É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã)")

        if len(records_to_write) > 0:
            self.write_results_in_batches(timeframe, periods, records_to_write, batch_days)
        else:
            logger.info(f"   ‚úÖ –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")

        logger.info(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ HV –¥–ª—è {timeframe} –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    def write_results_in_batches(
        self,
        timeframe: str,
        periods: List[int],
        df: pd.DataFrame,
        batch_days: int
    ):
        """
        –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã HV –≤ –ë–î –±–∞—Ç—á–∞–º–∏.

        Args:
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
            periods: –ü–µ—Ä–∏–æ–¥—ã HV
            df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            batch_days: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö
        """
        table_name = f'indicators_bybit_futures_{timeframe}'

        start_date = df['timestamp'].min()
        end_date = df['timestamp'].max()
        current_date = start_date

        total_days = (end_date - start_date).days + 1
        total_batches = (total_days + batch_days - 1) // batch_days

        with tqdm(total=total_batches, desc=f"{self.symbol} {timeframe} - –ó–∞–ø–∏—Å—å –≤ –ë–î") as pbar:
            batch_num = 0

            while current_date <= end_date:
                batch_end = min(current_date + timedelta(days=batch_days), end_date + timedelta(days=1))

                batch_df = df[(df['timestamp'] >= current_date) & (df['timestamp'] < batch_end)]

                if not batch_df.empty:
                    with self.db.get_connection() as conn:
                        cur = conn.cursor()

                        # –í—Å–µ HV –∫–æ–ª–æ–Ω–∫–∏
                        all_columns = [f'hv_{p}' for p in periods] + [
                            'hv_ratio_7_30', 'hv_percentile_7d', 'hv_percentile_90d'
                        ]

                        set_clauses = [f"{col} = data.{col}" for col in all_columns]

                        values = []
                        for _, row in batch_df.iterrows():
                            value_tuple = (self.symbol, row['timestamp'])
                            for col in all_columns:
                                val = row[col]
                                if pd.isna(val):
                                    value_tuple += (None,)
                                else:
                                    value_tuple += (float(val),)
                            values.append(value_tuple)

                        if values:
                            value_columns = ', '.join(all_columns)
                            placeholders = ', '.join(['%s'] * (2 + len(all_columns)))

                            update_query = f"""
                                UPDATE {table_name} t
                                SET {', '.join(set_clauses)}
                                FROM (VALUES {', '.join([f'({placeholders})' for _ in values])})
                                AS data(symbol, timestamp, {value_columns})
                                WHERE t.symbol = data.symbol::VARCHAR
                                AND t.timestamp = data.timestamp::TIMESTAMPTZ
                            """

                            flat_values = [item for value_tuple in values for item in value_tuple]

                            cur.execute(update_query, flat_values)
                            conn.commit()

                batch_num += 1
                pbar.set_description(f"{self.symbol} {timeframe} - –ë–∞—Ç—á {batch_num}/{total_batches}")
                pbar.update(1)

                current_date = batch_end

        logger.info(f"   ‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ {len(df):,} –∑–∞–ø–∏—Å–µ–π –≤ {table_name}")

    def run(self, timeframes: List[str] = None, batch_days: int = 1,
            start_date: Optional[datetime] = None):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É HV –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤.

        Args:
            timeframes: –°–ø–∏—Å–æ–∫ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            batch_days: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        """
        if not timeframes:
            timeframes = self.config.get('timeframes', ['1m'])

        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ HV Loader")
        logger.info(f"üìä –¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {timeframes}")
        logger.info(f"üéØ –°–∏–º–≤–æ–ª: {self.symbol}")
        logger.info(f"üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_days} –¥–Ω–µ–π")
        if start_date:
            logger.info(f"üìÖ –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞: {start_date}")

        for timeframe in timeframes:
            if timeframe not in self.timeframe_minutes:
                logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º: {timeframe}")
                continue

            self.process_timeframe(timeframe, batch_days, start_date)

        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description='Historical Volatility (HV) Loader',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python hv_loader.py                          # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
  python hv_loader.py --symbol BTCUSDT         # –¢–æ–ª—å–∫–æ –¥–ª—è BTCUSDT
  python hv_loader.py --timeframe 1h           # –¢–æ–ª—å–∫–æ —á–∞—Å–æ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
  python hv_loader.py --force-reload           # –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        """
    )

    parser.add_argument('--symbol', type=str, default=None,
                       help='–û–¥–Ω–∞ —Ç–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTCUSDT)')
    parser.add_argument('--symbols', type=str, default=None,
                       help='–ù–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é')
    parser.add_argument('--timeframe', type=str,
                       help='–û–¥–∏–Ω —Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--timeframes', type=str,
                       help='–ù–µ—Å–∫–æ–ª—å–∫–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é')
    parser.add_argument('--batch-days', type=int, default=1,
                       help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –≤ –¥–Ω—è—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--start-date', type=str,
                       help='–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD')
    parser.add_argument('--force-reload', action='store_true',
                       help='–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ—Å—á—ë—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö')

    args = parser.parse_args()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã
    if args.symbols:
        symbols = [s.strip() for s in args.symbols.split(',')]
    elif args.symbol:
        symbols = [args.symbol]
    else:
        config_path = os.path.join(os.path.dirname(__file__), 'indicators_config.yaml')
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                symbols = config.get('symbols', ['BTCUSDT'])
        else:
            symbols = ['BTCUSDT']

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
    timeframes = None
    if args.timeframe:
        timeframes = [args.timeframe]
    elif args.timeframes:
        timeframes = args.timeframes.split(',')

    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
    start_date = None
    if args.start_date:
        try:
            start_date = datetime.strptime(args.start_date, '%Y-%m-%d').replace(tzinfo=timezone.utc)
        except ValueError:
            logger.error(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {args.start_date}. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ YYYY-MM-DD")
            sys.exit(1)

    logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    start_time = time.time()

    total_symbols = len(symbols)
    for idx, symbol in enumerate(symbols, 1):
        if idx > 1:
            logger.info("")
            logger.info("=" * 80)

        logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∏–º–≤–æ–ª–∞: {symbol} [{idx}/{total_symbols}]")

        try:
            loader = HVLoader(symbol=symbol)
            loader.symbol_progress = f"[{idx}/{total_symbols}]"
            loader.force_reload = args.force_reload
            loader.run(timeframes, args.batch_days, start_date)
            logger.info(f"‚úÖ –°–∏–º–≤–æ–ª {symbol} –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
        except KeyboardInterrupt:
            logger.info("‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            sys.exit(0)
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol}: {e}")
            import traceback
            traceback.print_exc()
            continue

    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)

    logger.info(f"üéâ –í—Å–µ —Å–∏–º–≤–æ–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {symbols}")
    logger.info(f"‚è±Ô∏è  Total time: {minutes}m {seconds}s")


if __name__ == "__main__":
    main()
